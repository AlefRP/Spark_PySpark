{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFv4qiW9GnO+iHQ9iRCt5/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlefRP/Spark_PySpark/blob/main/pyspark_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PySpark in Google Colab**"
      ],
      "metadata": {
        "id": "v6NB7jfwYVgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing PySpark is not as straightforward as the usual process in Python. It involves more than just running a pip install. First, you need to install dependencies such as **Java 17** and **Apache Spark 3.5.1** along with **Hadoop 3.3**."
      ],
      "metadata": {
        "id": "1Yponzw2Ydzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!apt-get install openjdk-17-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "3EI53MKB7ho7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to set up the environment variables. This ensures that the Colab environment can correctly locate and use the installed dependencies."
      ],
      "metadata": {
        "id": "5FWZTasE7jRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To interact with the terminal and manipulate it, you can use the **os** library in Python."
      ],
      "metadata": {
        "id": "5_cLWiFW8fSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n",
        "\n",
        "# Make PySpark importable\n",
        "import findspark\n",
        "findspark.init('/content/spark-3.5.1-bin-hadoop3')"
      ],
      "metadata": {
        "id": "nTkx7B9T8iuh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With everything set up, let's run a local session to test if the installation worked correctly."
      ],
      "metadata": {
        "id": "RRmWX7kM7ux2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Introduction\").getOrCreate()\n",
        "\n",
        "# Test the Spark session\n",
        "df = spark.createDataFrame([(1, 'foo'), (2, 'bar')], ['id', 'label'])\n",
        "df.show()"
      ],
      "metadata": {
        "id": "Wvf3ZJ4Q-Y2i",
        "outputId": "32dc1a06-3ede-49c9-8f55-a91df448dd1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "| id|label|\n",
            "+---+-----+\n",
            "|  1|  foo|\n",
            "|  2|  bar|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1 Using Spark with Pythom**"
      ],
      "metadata": {
        "id": "XHcu41at9FQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step in using Spark is to connect to a cluster."
      ],
      "metadata": {
        "id": "oH_0dtfo-o6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a practical scenario, the cluster will be hosted on a remote machine connected to all other nodes. This setup includes a primary machine known as the master, responsible for distributing data and computations. The master communicates with the other machines in the cluster, known as workers. The master delegates tasks and data to the workers for processing, and they return the results to the master."
      ],
      "metadata": {
        "id": "LLeoW4ZK-xSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Creating a SparkSession**"
      ],
      "metadata": {
        "id": "bDk0U2Xs9iPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating multiple `SparkSessions` and `SparkContexts` can lead to issues, so it is a best practice to use the `SparkSession.builder.getOrCreate()` method. This method returns an existing `SparkSession` if one is already present in the environment, or it creates a new one if necessary. This approach ensures that you avoid problems associated with having multiple sessions or contexts running simultaneously."
      ],
      "metadata": {
        "id": "M1k3dilG9ocG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a local session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Introduction\").getOrCreate()"
      ],
      "metadata": {
        "id": "Dq94ugDxACLN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify SparkContext\n",
        "print(spark)\n",
        "\n",
        "# Print Spark version\n",
        "print(spark.version)"
      ],
      "metadata": {
        "id": "hvI-yUhfAFkP",
        "outputId": "88f7f8f6-dd24-4624-8bed-6e67a6be4826",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x78a9e46d4850>\n",
            "3.5.1\n"
          ]
        }
      ]
    }
  ]
}